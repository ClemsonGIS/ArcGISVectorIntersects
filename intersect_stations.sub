# This is a HTCondor submit process.
#
# The "universe" parameter allows you to specify a runtime environment that you want the job to run under.
# "vanilla" is the default value and usually a safest to use when environment isn't known. The "vanilla" value 
# doesn't allow a job to be checkpointed or migrated. The job must run to completion on a node or it will need to 
# be rerun.
universe                = vanilla

# The "executable" parameter specifys where the execution file resides. In this case we are running the python
# Interpreter "python.exe".
executable              = C:\Python27\ArcGIS10.3\python.exe

# The "transfer_executable" parameter allows you to transfer the program you want to run to the Condor node where 
# the job will run. For this process we specify "FALSE" because we expect the "python.exe" file to reside on every 
# node and at the exact path specified in the "executable" parameter on every node we submit this job to.
transfer_executable     = FALSE

# The "requirements" parameter allows you to specify what type of machine architecture and OS the program will run 
# under. We also created a flag called "HAS_ARCGIS" that allows you to say you only want machines with ArcGIS Desktop 
# installed to be eligbible for selection.
requirements            = Arch=="X86_64" && OpSys=="WINDOWS" && HAS_ARCGIS


# Create a pid variable that is process number + 1
pid = $(Process) + 1

# The "arguments" parameter says what arguments you want to pass to the executable file. 
# since the executable file is the python interpreter we need to pass, as one of the arguments, the python script
# we want to run. The arguments after the python script name are arguments for the python script itself. 
arguments               = intersect_stations.py Routes4_Pline_$INT(pid) Station_Points 

# The "transfer_input_files" parameter allows you to specify other addition files to be transferred to the node that 
# might be needed to run the job successfully.
transfer_input_files    = intersect_stations.py,Routes4_Pline_$INT(pid).gdb.zip

# The "should_transfer_files" parameter allows you to state you want all files specified on the "transfer_input_files"
# parameter to be sent to the remote node. 
should_transfer_files   = YES

# The "when_to_transfer_output" parameter says you want to transfer any files, that were created on the remote node, 
# back to the submit machine after the job is completed.
when_to_transfer_output = ON_EXIT

# The "initialdir" parameter states the path where the input files will be found. In this case the path is relative
# to the directory from where the submit process file was submitted. 
#initialdir              = area_of_solar_radiation

# The "output", "error", and "log" parameters specify the path, on the submitting machinge, where the logs will be created 
# and updated. In this case it's relative to the submit process file path.
output = logs/output-$INT(pid).txt
error  = logs/error-$INT(pid).txt
log    = logs/log.txt

# The Queue command initiates the submission to the remote nodes. The number 600 means that we want to submit this job 600 times. 
# Each submission will go to a different node.
queue 
